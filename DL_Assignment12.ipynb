{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "023b200e",
   "metadata": {},
   "source": [
    "1. How does unsqueeze help us to solve certain broadcasting problems?\n",
    "\n",
    "2. How can we use indexing to do the same operation as unsqueeze?\n",
    "\n",
    "3. How do we show the actual contents of the memory used for a tensor?\n",
    "\n",
    "4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added\n",
    "to each row or each column of the matrix? (Be sure to check your answer by running this\n",
    "code in a notebook.)\n",
    "\n",
    "5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "\n",
    "6. Implement matmul using Einstein summation.\n",
    "\n",
    "7. What does a repeated index letter represent on the lefthand side of einsum?\n",
    "\n",
    "8. What are the three rules of Einstein summation notation? Why?\n",
    "\n",
    "9. What are the forward pass and backward pass of a neural network?\n",
    "\n",
    "10. Why do we need to store some of the activations calculated for intermediate layers in the\n",
    "forward pass?\n",
    "\n",
    "11. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "\n",
    "12. How can weight initialization help avoid this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d757138",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da970fb",
   "metadata": {},
   "source": [
    "1. unsqueeze helps solve certain broadcasting problems by adding a new dimension to a tensor, which can be used to match the dimensions of other tensors in element-wise operations. For example, if you have a 1D tensor and want to perform element-wise operations with a 2D tensor, you can use unsqueeze to add a new dimension and make their shapes compatible for broadcasting.\n",
    "\n",
    "2. You can achieve the same operation as unsqueeze using indexing. For example, if you want to add a new dimension to a tensor a, you can use a[:, None] or a[:, np.newaxis] to add a new dimension along the second axis. This effectively replicates the data along that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a09bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanmay763144\\AppData\\Local\\Temp\\ipykernel_4428\\1598301118.py:6: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = tensor.storage()\n"
     ]
    }
   ],
   "source": [
    "#3 To show the actual contents of the memory used for a tensor, you can use the .storage() method in PyTorch.\n",
    "\n",
    "import torch\n",
    "\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "storage = tensor.storage()\n",
    "print(storage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0daa50",
   "metadata": {},
   "source": [
    "4. When adding a vector of size 3 to a matrix of size 3×3, the elements of the vector are added to each column of the matrix.\n",
    "\n",
    "5. Broadcasting and expand_as do not result in increased memory use. They are memory-efficient because they do not create new copies of data but rather allow operations to be performed on the original data in a memory-efficient way by virtually expanding or broadcasting the dimensions as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3215634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32]])\n"
     ]
    }
   ],
   "source": [
    "#6 Implementing matmul using Einstein summation:\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3]])\n",
    "b = torch.tensor([[4], [5], [6]])\n",
    "\n",
    "result = torch.einsum('ij,jk->ik', a, b)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5c56bc",
   "metadata": {},
   "source": [
    "7. A repeated index letter on the left-hand side of einsum represents summation or contraction along that dimension. For example, in 'ij,jk->ik', the repeated index letter 'j' means that the 'j' dimension of the first input tensor and the second input tensor will be summed over.\n",
    "\n",
    "8. The three rules of Einstein summation notation are:\n",
    "\n",
    "Repeated indices imply summation.\n",
    "Indices that appear only once are free indices.\n",
    "The resulting expression specifies the dimensions of the output tensor.\n",
    "These rules help define how tensors are contracted or operated on in an einsum expression.\n",
    "\n",
    "9. The forward pass of a neural network involves feeding input data through the network's layers, computing intermediate activations, and ultimately producing an output. The backward pass, also known as backpropagation, is the process of calculating gradients of the loss function with respect to the network's parameters. It is essential for updating the network's weights during training.\n",
    "\n",
    "10. Storing activations for intermediate layers in the forward pass is necessary for backpropagation. During backpropagation, the gradients are computed by propagating them backward through the network. The stored intermediate activations are needed to compute these gradients efficiently.\n",
    "\n",
    "11.The downside of having activations with a standard deviation too far away from 1 is that it can lead to issues like vanishing or exploding gradients during backpropagation. This can make training difficult or slow, as the gradients may become very small or very large, affecting the weight updates.\n",
    "\n",
    "12. Weight initialization can help avoid the problem of activations with a standard deviation far from 1 by setting the initial values of weights to specific values. For example, initializing weights with values drawn from a suitable distribution (e.g., Xavier/Glorot initialization) can help ensure that activations have an appropriate scale, reducing the risk of vanishing or exploding gradients during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff093dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
