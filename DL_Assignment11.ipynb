{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc3a364",
   "metadata": {},
   "source": [
    "1. Write the Python code to implement a single neuron.\n",
    "\n",
    "2. Write the Python code to implement ReLU.\n",
    "\n",
    "3. Write the Python code for a dense layer in terms of matrix multiplication.\n",
    "\n",
    "4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions\n",
    "   and functionality built into Python).\n",
    "\n",
    "5. What is the “hidden size” of a layer?\n",
    "\n",
    "6. What does the t method do in PyTorch?\n",
    "\n",
    "7. Why is matrix multiplication written in plain Python very slow?\n",
    "\n",
    "8. In matmul, why is ac==br?\n",
    "\n",
    "9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "\n",
    "10. What is elementwise arithmetic?\n",
    "\n",
    "11. Write the PyTorch code to test whether every element of a is greater than the\n",
    "    corresponding element of b.\n",
    "\n",
    "12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "\n",
    "13. How does elementwise arithmetic help us speed up matmul?\n",
    "\n",
    "14. What are the broadcasting rules?\n",
    "\n",
    "15. What is expand_as? Show an example of how it can be used to match the results of\n",
    "     broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d84d2",
   "metadata": {},
   "source": [
    "# Answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d69a3b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "def single_neuron(input_data, weights, bias):\n",
    "    weighted_sum = sum(x * w for x, w in zip(input_data, weights))\n",
    "    output = weighted_sum + bias\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935f4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "def relu(x):\n",
    "    return max(0, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d7ef7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "import numpy as np\n",
    "\n",
    "def dense_layer(input_data, weights, bias):\n",
    "    output = np.dot(input_data, weights) + bias\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d304d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "def dense_layer(input_data, weights, bias):\n",
    "    output = [sum(x * w for x, w in zip(input_data, weights[i])) + bias[i] for i in range(len(weights))]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4269ae9e",
   "metadata": {},
   "source": [
    "5. The \"hidden size\" of a layer refers to the number of neurons or units in that layer. It represents the dimensionality of the layer's output.\n",
    "\n",
    "6. In PyTorch, the t method is used to transpose a tensor. It swaps the dimensions of the tensor.\n",
    "\n",
    "7. Matrix multiplication in plain Python is slow because it involves using nested loops to iterate over the elements of matrices and perform multiplication and addition for each element. This approach is not optimized for performance and becomes inefficient for large matrices.\n",
    "\n",
    "8. In matrix multiplication (matmul), ac must be equal to br for the multiplication to be valid. a is an m x n matrix, and b is an n x p matrix. The resulting matrix will be m x p.\n",
    "\n",
    "9. In Jupyter Notebook, you can measure the time taken for a single cell to execute using the %timeit magic command:\n",
    "\n",
    "\n",
    "   %timeit your_function_or_code_here()\n",
    "   \n",
    "10. Elementwise arithmetic refers to performing arithmetic operations (addition, subtraction, multiplication, division, etc.) between corresponding elements of two or more arrays or tensors. It operates on each element independently without any cross-element interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a5c2213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True, False, False])\n"
     ]
    }
   ],
   "source": [
    "#11 \n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 2, 4])\n",
    "result = a > b\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5be1e4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "#12 A rank-0 tensor is essentially a scalar, a single value without any dimensions. \n",
    "#   You can convert it to a plain Python data type using the .item() method in PyTorch. For example:\n",
    "\n",
    "import torch\n",
    "\n",
    "tensor_scalar = torch.tensor(42)\n",
    "python_scalar = tensor_scalar.item()\n",
    "print(python_scalar)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e199e274",
   "metadata": {},
   "source": [
    "13. Elementwise arithmetic helps speed up matmul by allowing parallelization and optimized low-level operations when implemented using libraries like NumPy or optimized hardware, such as GPUs.\n",
    "\n",
    "14. Broadcasting rules in array operations allow arrays of different shapes to be combined or used together in elementwise operations. Broadcasting automatically stretches or duplicates the smaller array to match the shape of the larger array to perform elementwise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "374fdebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11, 12, 13])\n"
     ]
    }
   ],
   "source": [
    "# expand_as is a PyTorch method that expands the dimensions of a tensor to match the size of another tensor. \n",
    "# Here's an example of how it can be used to match the results of broadcasting:\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([10])\n",
    "result = a + b.expand_as(a)\n",
    "print(result)\n",
    "#In this example, expand_as is used to make b have the same shape as a for elementwise addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84965df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
